{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d7871",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchvision --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7045de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64676fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "GPU name: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c824db07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Comprehensive GPU Benchmark\n",
      "============================================================\n",
      "============================================================\n",
      "SYSTEM INFORMATION\n",
      "============================================================\n",
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "GPU: NVIDIA GeForce GTX 1660 Ti\n",
      "GPU Memory: 6.00 GB\n",
      "GPU Compute capability: (7, 5)\n",
      "Current device: 0\n",
      "Device count: 1\n",
      "Using device: cuda\n",
      "============================================================\n",
      "\n",
      "Tensor Operations Benchmark\n",
      "----------------------------------------\n",
      "Matrix Multiplication: 0.000835 seconds\n",
      "Element-wise Multiplication: 0.000100 seconds\n",
      "Matrix Transpose + Multiplication: 0.000889 seconds\n",
      "SVD Decomposition: 0.153929 seconds\n",
      "Matrix Inverse: 0.006971 seconds\n",
      "\n",
      "Neural Network Benchmark (batch_size=32)\n",
      "--------------------------------------------------\n",
      "Training time per epoch: 0.0038 seconds\n",
      "Inference time per batch: 0.000242 seconds\n",
      "Peak GPU memory usage: 46.55 MB\n",
      "\n",
      "Memory Bandwidth Test\n",
      "------------------------------\n",
      "Matrix 1024x1024: 1372.07 GFLOPS\n",
      "Matrix 2048x2048: 3668.14 GFLOPS\n",
      "Matrix 4096x4096: 5614.92 GFLOPS\n",
      "Matrix 8192x8192: 4945.43 GFLOPS\n",
      "\n",
      "============================================================\n",
      "BENCHMARK COMPLETED!\n",
      "============================================================\n",
      "Average GPU Performance: 3900.14 GFLOPS\n",
      "Neural Network Training Speed: 0.0038 s/epoch\n",
      "Inference Speed: 0.000242 s/batch\n",
      "\n",
      "Testing Mixed Precision Training...\n",
      "Mixed precision test passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "class BenchmarkModel(nn.Module):\n",
    "    \"\"\"Тестовая модель для benchmarking\"\"\"\n",
    "    def __init__(self, input_size: int = 512, hidden_size: int = 1024, num_layers: int = 3):\n",
    "        super(BenchmarkModel, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        # Входной слой\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(0.1))\n",
    "        \n",
    "        # Скрытые слои\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(0.1))\n",
    "        \n",
    "        # Выходной слой\n",
    "        self.layers.append(nn.Linear(hidden_size, 10))\n",
    "        self.layers.append(nn.LogSoftmax(dim=1))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class GPUBenchmark:\n",
    "    \"\"\"Комплексный бенчмарк для GPU\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.results = {}\n",
    "    \n",
    "    def print_system_info(self) -> None:\n",
    "        \"\"\"Вывод информации о системе\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"SYSTEM INFORMATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"CUDA version: {torch.version.cuda}\")\n",
    "            print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "            print(f\"GPU Compute capability: {torch.cuda.get_device_capability()}\")\n",
    "            print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "            print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    def tensor_operations_benchmark(self, size: Tuple[int, int] = (1024, 1024)) -> Dict[str, float]:\n",
    "        \"\"\"Тест операций с тензорами\"\"\"\n",
    "        print(\"\\nTensor Operations Benchmark\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Создаем большие тензоры\n",
    "        a = torch.randn(*size, device=self.device)\n",
    "        b = torch.randn(*size, device=self.device)\n",
    "        \n",
    "        operations = {\n",
    "            'Matrix Multiplication': lambda: torch.mm(a, b),\n",
    "            'Element-wise Multiplication': lambda: a * b,\n",
    "            'Matrix Transpose + Multiplication': lambda: torch.mm(a, b.t()),\n",
    "            'SVD Decomposition': lambda: torch.svd(a),\n",
    "            'Matrix Inverse': lambda: torch.inverse(a),\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for op_name, op_func in operations.items():\n",
    "            # Прогрев\n",
    "            for _ in range(3):\n",
    "                op_func()\n",
    "            \n",
    "            torch.cuda.synchronize()  # Ждем завершения всех операций на GPU\n",
    "            \n",
    "            start_time = time.time()\n",
    "            for _ in range(10):  # 10 итераций для стабильности\n",
    "                result = op_func()\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            avg_time = (end_time - start_time) / 10\n",
    "            results[op_name] = avg_time\n",
    "            print(f\"{op_name}: {avg_time:.6f} seconds\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def neural_network_benchmark(self, batch_size: int = 32, input_size: int = 512) -> Dict[str, float]:\n",
    "        \"\"\"Тест производительности нейросети\"\"\"\n",
    "        print(f\"\\nNeural Network Benchmark (batch_size={batch_size})\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        model = BenchmarkModel(input_size=input_size).to(self.device)\n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Создаем тестовые данные\n",
    "        x = torch.randn(batch_size, input_size, device=self.device)\n",
    "        y = torch.randint(0, 10, (batch_size,), device=self.device)\n",
    "        \n",
    "        # Прогрев\n",
    "        for _ in range(3):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Замер времени обучения\n",
    "        start_time = time.time()\n",
    "        for _ in range(50):  # 50 итераций\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Замер времени inference\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            start_time = time.time()\n",
    "            for _ in range(100):  # 100 предсказаний\n",
    "                _ = model(x)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            inference_time = time.time() - start_time\n",
    "        \n",
    "        results = {\n",
    "            'Training_time_per_epoch': training_time / 50,\n",
    "            'Inference_time_per_batch': inference_time / 100,\n",
    "            'Total_training_memory': torch.cuda.max_memory_allocated() / 1024**2\n",
    "        }\n",
    "        \n",
    "        print(f\"Training time per epoch: {results['Training_time_per_epoch']:.4f} seconds\")\n",
    "        print(f\"Inference time per batch: {results['Inference_time_per_batch']:.6f} seconds\")\n",
    "        print(f\"Peak GPU memory usage: {results['Total_training_memory']:.2f} MB\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def memory_bandwidth_test(self) -> Dict[str, float]:\n",
    "        \"\"\"Тест пропускной способности памяти\"\"\"\n",
    "        print(\"\\nMemory Bandwidth Test\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        sizes = [1024, 2048, 4096, 8192]  # Разные размеры матриц\n",
    "        results = {}\n",
    "        \n",
    "        for size in sizes:\n",
    "            a = torch.randn(size, size, device=self.device)\n",
    "            b = torch.randn(size, size, device=self.device)\n",
    "            \n",
    "            # Прогрев\n",
    "            torch.mm(a, b)\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            for _ in range(10):\n",
    "                c = torch.mm(a, b)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Вычисляем пропускную способность\n",
    "            operations = 2 * size ** 3  # Примерное количество операций для умножения матриц\n",
    "            time_taken = (end_time - start_time) / 10\n",
    "            gflops = (operations / time_taken) / 1e9\n",
    "            \n",
    "            results[f'Matrix_{size}x{size}'] = gflops\n",
    "            print(f\"Matrix {size}x{size}: {gflops:.2f} GFLOPS\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_comprehensive_benchmark(self) -> Dict[str, Any]:\n",
    "        \"\"\"Запуск комплексного тестирования\"\"\"\n",
    "        print(\"Starting Comprehensive GPU Benchmark\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        self.print_system_info()\n",
    "        \n",
    "        results = {\n",
    "            'system_info': {\n",
    "                'pytorch_version': torch.__version__,\n",
    "                'cuda_available': torch.cuda.is_available(),\n",
    "                'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None',\n",
    "                'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0\n",
    "            },\n",
    "            'tensor_operations': self.tensor_operations_benchmark(),\n",
    "            'neural_network': self.neural_network_benchmark(),\n",
    "            'memory_bandwidth': self.memory_bandwidth_test()\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"BENCHMARK COMPLETED!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Сводка результатов\n",
    "        if torch.cuda.is_available():\n",
    "            avg_gflops = np.mean(list(results['memory_bandwidth'].values()))\n",
    "            print(f\"Average GPU Performance: {avg_gflops:.2f} GFLOPS\")\n",
    "            print(f\"Neural Network Training Speed: {results['neural_network']['Training_time_per_epoch']:.4f} s/epoch\")\n",
    "            print(f\"Inference Speed: {results['neural_network']['Inference_time_per_batch']:.6f} s/batch\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Запуск бенчмарка\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark = GPUBenchmark()\n",
    "    results = benchmark.run_comprehensive_benchmark()\n",
    "    \n",
    "    # Дополнительная проверка смешанной точности (едоступно)\n",
    "    if torch.cuda.is_available() and hasattr(torch, 'amp'):\n",
    "        print(\"\\nTesting Mixed Precision Training...\")\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            # Быстрый тест mixed precision\n",
    "            x = torch.randn(32, 512, device='cuda')\n",
    "            model = BenchmarkModel().cuda()\n",
    "            output = model(x)\n",
    "            print(\"Mixed precision test passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
